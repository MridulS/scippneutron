Notes on data streaming

All data sent to kafka are  via flat buffer

Scipp:
 - run start stop messages
 - event data
 - meta data
 - shopper timestamps

Each type has a schemas


Repo ess-dmsc/streaming_data_types
table in README for schemas marks some a obsolete

pl72_run_start:
 - nexus_strcture: any info known at the start of the run (e.g. geometry)

6s4t_run_stop:
 - job_id must match run_start

ev42_events:
 - all in nanoseconds, including tof
 - source_name: used to filter what data to get

f142_logdata:
 - used for PV updates
 - usually just a value and a timestamp

senv_data.fbs:
 - used to send sample environment directly to kafka instead of going via epics
 - 

tdct_timestamps:
 - chopper timestamps



---------------------------------------------------

Scippneutron code:

data_stream.py/data_stream(): -> makes a generator

 - usually don't use `topics`, instead use:

 - run_info_topic:
   - go and find last run_start message and use that to get info on what topics to listen to

 - Buffer sizes:
   - not easy to resize scipp data structures, so allocate buffers
   - good default, will see later what needs to be tweaked.

 - interval_argument: frequency on which to "yield"

 - start_time: can go back to the start of the run, even if data_stream is started after or during the run.
   it finds the start time in the last run_start message

 - how long is data kept on kafka? at ISIS data was kept for months on kafka.
   At ESS, might be just hours or minutes


 - Calls _data_stream: this is done for unit testing



_data_stream():

 - 1st thing: find the run start message: go backwards and find a run start message
 - get_run_start_message(): might need some filtering on instrument name since all start message may end up in the same place
 - ECDC has discussed having separate topics for each instrument

 - stream_info: info on where to look for other data

 - data from that start message is yielded as the first chunk of data
 - should in principle not look any different from any other chunk of data, right now it seems to be different? in st
 - other chunks could contain geometry again, e.g. if some pixels have moved
 - also first chunk of data most likely will not contain events
 - metadata (e.g. sample env) might be empty if values have not changed
 - so you may have empty data, or empty geometry, but should all have a general structure, with everything optional

 - then start separate process using python multiprocessing.
 - everything sent to mP must be pickleable (arguments or stuff sent to the queue), which is why there are some conversions from scipp values to float
 - when testing, if something goes wrong, the while loop on L241 could become infinite. So we introduce a timeout on L236.
 - passing queues to different processes
 - warnings are passed back to the main process, if not they go unnoticed in tests?
 - pickling: converting back to scipp: from_dict doesn't work well with DataArrays inside Variable

 - queue: data_queue
 - worker_instruction_queue: goes the other way than data_queue, for e.g. run stop message


- data_consumption_manager gets launched in the separate process: look at buffers and consumers
- both consumers and buffers have threads, from the Threads module, not processes from mP

consumer.py
- consumer: enum for real or fake consumer, fake for testing. get messages from test_message_queue. real consumer gets messages from kafka
- enable.auto.commit: usually reports to kafka broker where it go to in the topic, in case of a crash. This is not needed and has been disabled.
- message.max.size: default is 1MB. set to 100MB (need large size for json min start message). Seems to be no real impact on performance/latency
- if a message exceeds this size: would see an error in nicos
- size doesn't necessailry have to agree between broker and consumer
- KafkaConsumer: 'enable.partition.eof' : relates to stop message
- offsets: integers: index/position in log (not a number of byes)

data_buffer.py:
- StreamedDataBuffer: owns event data buffer. also metadat buffers -> maybe that should be moved out
- event weights are always 1: this is also true for data in nexus files
- init_metadata_buffers: we don't know what metadata we will get, we receive it dynamically from run start message
- SLOW: pv updates, FAST: sample env, CHOPPER timestamps
- thread is in an infinite loop _emit_loop()
- _emit_data: put the events as pickleable dict in output queue
- new_data(): callback sent to consumers: try all options and see which one works. could grab the first 4 bytes to determine which one to use. But try/except is easy and pythonic.
- _handle_event_data(): returns named tuples
- 







